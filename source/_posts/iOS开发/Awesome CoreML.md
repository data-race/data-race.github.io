---
title: Awesome CoreML
tags:
  - ioså¼€å‘
categories:
  - iOSå¼€å‘
date: 2023-07-17 20:17:29
---
#ioså¼€å‘ 


## æ¦‚è¦
ç¬”è®°ä¸­æ•´ç†äº†ä¸€äº›å…³äºCoreMLï¼ŒARKitï¼ŒMapKitçš„åº”ç”¨ã€‚åœ¨è¿è¡Œè¿™äº›é¡¹ç›®æ—¶ï¼Œå¾€å¾€éœ€è¦ç¼–è¾‘è¿™äº›é¡¹ç›®çš„çš„Bundle Identifierï¼Œä»¥åŠè®¾ç½®å¼€å‘è€…çš„Accountã€‚
![](img/6BEFDA89-DEFA-4002-AD5D-691F46847678.png
)

## 1. æ–‡æœ¬æ£€æµ‹åº”ç”¨
+ é¡¹ç›®åœ°å€ï¼šhttps://github.com/tucan9389/TextDetection-CoreML
+ å…³é”®å­—ï¼š Visionï¼ŒTextDetect
+ é¡¹ç›®æ•ˆæœï¼š
![](img/7DEFFBBA-FB5F-4CF0-89DC-118E81B967D1.png
)

+ åŠŸèƒ½æè¿°ï¼šç»˜åˆ¶å‡ºå½“å‰æ‘„åƒå¤´æ•æ‰çš„è§†é¢‘ä¸­æ‰€å­˜åœ¨æ–‡æœ¬çš„åŒºåŸŸã€‚
+ æŠ€æœ¯æè¿°ï¼šä¸»è¦ç”¨åˆ°äº†Visionä¸­è‡ªå¸¦çš„æ–‡æœ¬æ£€æµ‹æ¨¡å‹ï¼Œæ²¡æœ‰ä½¿ç”¨è‡ªè®­ç»ƒçš„æ•°æ®é›†å’Œæ¨¡å‹ã€‚é€šè¿‡å‘
VNImageRequestHandlerå‘èµ·VNDetectTextRectanglesRequestçš„è¯·æ±‚ï¼Œå½“è¯·æ±‚æˆåŠŸæ—¶ï¼Œä¼šè¿”å›ä¸€ä¸ªresultï¼Œresultçš„ç±»å‹æ˜¯VNTextObservationçš„æ•°ç»„
```
<VNTextObservation: 0x2836edd60> EEB346A3-3658-450A-AEF6-394309643B90 requestRevision=1 confidence=1.000000 boundingBox=[0.0812656, 0.4079, 0.814308, 0.104492], 
<VNTextObservation: 0x2836ed360> 4A47D180-8C4E-48F3-AEE0-2E9EB75BCA49 requestRevision=1 confidence=1.000000 boundingBox=[0.00438134, 0.314129, 0.614648, 0.0842859], <VNTextObservation: 0x2836edea0> 7F74A257-89B1-43AC-B6C9-D185A2066D39 requestRevision=1 confidence=1.000000 boundingBox=[0.647553, 0.37663, 0.23406, 0.0514283],
...
```
æ¯ä¸€é¡¹éƒ½æ˜¯ä¸€ä¸ªVNTextObservationï¼Œç»™å‡ºäº†å‡ºç°æ–‡æœ¬åŒºåŸŸçš„boundingBoxï¼Œä»¥åŠæ¨æ–­çš„ç½®ä¿¡åº¦confidenceç­‰ä¿¡æ¯ã€‚åœ¨ä¹‹å‰è®¾ç½®requestæ—¶ï¼Œè®¾ç½®äº†å½“è¯·æ±‚ç»“æŸåçš„å›è°ƒå‡½æ•°:
``` swift
// MARK: - Setup Core ML
    func setUpModel() {
        let request = VNDetectTextRectanglesRequest(completionHandler: self.visionRequestDidComplete)
        request.reportCharacterBoxes = true
        self.request = request
    }
```
è¯·æ±‚ç»“æŸæ—¶ä¼šå›è°ƒvisonRequestDidComplete()ï¼Œè¿™ä¸ªå‡½æ•°çš„ä»£ç é‡Œç”¨äº†å¾ˆå¤šemojiçš„ç¬¦å·
``` swift
func visionRequestDidComplete(request: VNRequest, error: Error?) {
        self.ğŸ‘¨â€ğŸ”§.ğŸ·(with: "endInference")
        guard let observations = request.results else {
            // end of measure
            self.ğŸ‘¨â€ğŸ”§.ğŸ¬ğŸ¤š()
            return
        }
        print(observations)
        DispatchQueue.main.async {
            let regions: [VNTextObservation?] = observations.map({$0 as? VNTextObservation})
            
            self.drawingView.regions = regions
            
            // end of measure
            self.ğŸ‘¨â€ğŸ”§.ğŸ¬ğŸ¤š()
        }
    }
```
å¤§æ¦‚çš„æ„æ€æ˜¯å°†resultä¸­çš„VNTextObservationæ•°ç»„æå–å‡ºæ¥ï¼Œç„¶åèµ‹ç»™drawingView.regionsï¼ŒdrawingViewæ˜¯å’ŒvideoPreviewé‡åˆçš„ä¸€ä¸ªUIViewï¼Œç”¨æ¥ç»˜åˆ¶æ–‡æœ¬åŒºåŸŸã€‚æœ€ååœ¨drawingViewä¸­å®Œæˆå¯¹æ–‡æœ¬åŒºåŸŸçš„ç»˜åˆ¶ã€‚

+ é¡¹ç›®æ€»ç»“ï¼šè¯¥é¡¹ç›®è¾ƒä¸ºç®€å•ï¼Œç”¨åˆ°äº†åŸºç¡€çš„æ–‡æœ¬æ£€æµ‹æ–¹æ³•ã€‚ä¸»è¦çš„é€»è¾‘å’Œä¹‹å‰çš„é›¶é£Ÿåˆ†ç±»ç±»ä¼¼ï¼Œåªåœ¨è¯·æ±‚çš„ç±»å‹å’Œæ¨¡å‹ä¸Šæœ‰åŒºåˆ«ã€‚


## 2.  å›¾åƒåˆ†å‰²
+ é¡¹ç›®åœ°å€ï¼šhttps://github.com/tucan9389/PoseEstimation-CoreML
+ å…³é”®å­—ï¼š Coreml, Vison, å›¾åƒåˆ†å‰²
+ é¡¹ç›®æ•ˆæœï¼š
![](img/5A2C4E23-7633-4C55-92A3-5CA694A7C724.png
)
+ åŠŸèƒ½æè¿°ï¼šå°†å›¾ç‰‡åˆ†å‰²ä¸ºè‹¥å¹²è‡ªå®šä¹‰çš„éƒ¨åˆ†ã€‚
+ æŠ€æœ¯æè¿°ï¼šä½¿ç”¨äº†Appleæä¾›çš„DeepLabV3å›¾åƒåˆ†å‰²æ¨¡å‹([Machine Learning - Models - Apple Developer](https://developer.apple.com/machine-learning/models/))ï¼Œè¯¥æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ª513 * 513å¤§å°çš„å›¾åƒï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ª513 * 513å¤§å°çš„Intå‹çŸ©é˜µï¼Œæ¯ä¸ªIntå€¼ä»£è¡¨è¿™ä¸ªåƒç´ å±äºå“ªç±»ç‰©ä½“ã€‚åº”ç”¨å¯ä»¥å¯¹è§†é¢‘è¿›è¡Œå®æ—¶åˆ†å‰²ï¼Œä¹Ÿå¯ä»¥å¯¹é™æ€å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚è¿™ä¸ªåº”ç”¨å’Œä¸Šä¸€ä¸ªæ–‡æœ¬æ£€æµ‹åº”ç”¨çš„ä½œè€…ä¸€æ ·ï¼Œæ‰€ä»¥ä»£ç çš„é€»è¾‘åŸºæœ¬ä¸€æ ·ï¼Œåªæ˜¯ä¿®æ”¹äº†ä¸€äº›å’Œæ¨¡å‹æœ‰å…³çš„ä»£ç ã€‚
+ é¡¹ç›®æ€»ç»“ï¼š è¯¥é¡¹ç›®ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå’Œä¸Šä¸€ä¸ªé¡¹ç›®ç›¸æ¯”ä½¿ç”¨äº†é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯ä»¥å­¦ä¹ å¦‚ä½•åœ¨ä¸€ä¸ªxcodeé¡¹ç›®ä¸­å¯¼å…¥æ¨¡å‹æ–‡ä»¶ã€‚æ­¤å¤–è¯¥é¡¹ç›®ä½¿ç”¨TabBarControlleræ¥ç»„ç»‡å¤šä¸ªé¡µé¢ï¼Œéœ€è¦æ›´å¤šæœ‰å…³UIè®¾è®¡çš„çŸ¥è¯†ã€‚


## 3. å›¾åƒæ·±åº¦æ£€æµ‹
+ é¡¹ç›®åœ°å€ï¼šhttps://github.com/tucan9389/DepthPrediction-CoreML
+ å…³é”®å­—ï¼š CoreMLï¼ŒVisionï¼Œæ·±åº¦æ£€æµ‹
+ é¡¹ç›®æ•ˆæœï¼š
![](img/1DB2409B-16DF-4350-9D8B-6333A1A3ED41.png
)

+ åŠŸèƒ½æè¿°ï¼šæ£€æµ‹å›¾åƒçš„æ™¯æ·±ä¿¡æ¯ã€‚
+ æŠ€æœ¯æè¿°ï¼šä½¿ç”¨äº†Appleæä¾›çš„æ™¯æ·±æ£€æµ‹æ¨¡å‹FCRN([Machine Learning - Models - Apple Developer](https://developer.apple.com/machine-learning/models/))
ï¼Œè¯¥æ¨¡å‹çš„è¾“å…¥æ˜¯304 * 228å¤§å°çš„å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªdoubleå‹çŸ©é˜µï¼Œå¤§å°å¤§æ¦‚æ˜¯åŸå§‹å›¾ç‰‡çš„ä¸€åŠ(128 * 160)ï¼Œå€¼è¶Šå¤§æ„å‘³ç€åœ¨åŸå§‹å›¾ç‰‡ä¸­ç‰©ä½“çš„æ™¯æ·±è¶Šæ·±ã€‚åº”ç”¨å…·ä½“çš„é€»è¾‘å’Œä¹‹å‰ä¸¤ä¸ªåº”ç”¨ç±»ä¼¼ã€‚

+ é¡¹ç›®æ€»ç»“ï¼šè¯¥é¡¹ç›®è¾ƒä¸ºç®€å•ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚å¯ä»¥å¸®åŠ©å­¦ä¹ åœ¨åº”ç”¨ä¸­ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹çš„æµç¨‹ã€‚


## 4.  å›¾åƒé£æ ¼è¿ç§»ï¼š è¾¹ç¼˜æå–
+ é¡¹ç›®åœ°å€ï¼š https://github.com/s1ddok/HED-CoreML
+ å…³é”®å­—ï¼šCoreMLï¼ŒVisionï¼Œ é£æ ¼è¿ç§»
+ é¡¹ç›®æ•ˆæœï¼š ![[68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a496d5073484d6836385a4e505979775f7561534239672e706e67.png]]

+ åŠŸèƒ½æè¿°ï¼šé€šè¿‡å‡ ç§ä¸åŒçš„æ¨¡å‹ï¼Œå¯¹åŸå§‹å›¾ç‰‡è¿›è¡Œè¾¹ç¼˜æå–
+ æŠ€æœ¯æè¿°ï¼šä½¿ç”¨è‡ªå·±è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œè¾¹ç¼˜æå–ã€‚æ¨¡å‹çš„ä¿¡æ¯å¦‚ä¸‹ï¼š
![](img/830C5F03-A4F1-4795-9F0A-D4BACDD3A645.png
)
è¾“å‡ºæœ‰5ä¸ªArrayï¼Œå¯¹åº”ç”¨æˆ·é€‰æ‹©çš„5ç§ä¸åŒçš„æ¨¡å‹ã€‚æ¯ä¸ªArrayçš„ç±»å‹æ˜¯Double 1 x 1 x 1 x 500 x 500 arrayï¼Œå’Œè¾“å…¥æ˜¯ç›¸åŒçš„ã€‚
ç„¶åå¯¹æ¯ä¸ªè¾“å‡ºå€¼ç”¨sigmoidè¿›è¡Œæ¿€æ´»ï¼Œè½¬æ¢æˆç°åº¦å›¾ï¼š
``` swift
// Normalize result features by applying sigmoid to every pixel and convert to UInt8
        for i in 0..<inputW {
            for j in 0..<inputH {
                let idx = i * inputW + j
                let value = dataPointer[idx]
                
                let sigmoid = { (input: Double) -> Double in
                    return 1 / (1 + exp(-input))
                }
                
                let result = sigmoid(value)
                imgData[idx] = UInt8(result * 255)
            }
        }
```

+ é¡¹ç›®æ€»ç»“ï¼šä¹Ÿæ˜¯æ¯”è¾ƒå¸¸è§„çš„é¡¹ç›®ã€‚

## 5.  å›¾åƒç‰©ä½“æ£€æµ‹(YOLO: ä¸€ç§ç‰©ä½“æ£€æµ‹ç¥ç»ç½‘ç»œ)
+ é¡¹ç›®åœ°å€: https://github.com/hollance/YOLO-CoreML-MPSNNGraph
+ å…³é”®è¯ï¼š CoreML, Vision,  Keras, coremltools, å›¾åƒç‰©ä½“æ£€æµ‹
+ é¡¹ç›®æ•ˆæœï¼š
![](img/780D2A81-397C-4BFE-979B-1F68A0E921A4.png
)

+ åŠŸèƒ½æè¿°ï¼š  å®æ—¶åœ°ä»æ•æ‰åˆ°çš„å›¾åƒä¸­æ£€æµ‹å‡ºåŒ…æ‹¬äººã€æ±½è½¦ã€çŒ«ã€ç‹—ç­‰åœ¨å†…çš„å¤šç§ç‰©ä½“ã€‚
+ æŠ€æœ¯æè¿°ï¼š  è¯¥åº”ç”¨å¤ç°äº†ä¸€ç§ç”¨äºç‰©ä½“æ£€æµ‹çš„ç¥ç»ç½‘ç»œYOLOï¼Œè¿™ä¸ªç½‘ç»œååˆ†å¤æ‚ã€‚ä½¿ç”¨Kerasè¿›è¡Œè®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨coremltoolså°†Kerasçš„æ¨¡å‹è½¬åŒ–ä¸ºæ‰€éœ€çš„mlmodelæ ¼å¼æ¨¡å‹ã€‚ä½¿ç”¨çš„æ•°æ®é›†æ˜¯http://host.robots.ox.ac.uk/pascal/VOC/ã€‚æœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹çš„è¾“å…¥æ˜¯ï¼š416*416å¤§å°çš„å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯125*13*13å¤§å°çš„Arrayï¼Œç»™å‡ºäº†ç‰©ä½“çš„ä½ç½®ç­‰ä¿¡æ¯ã€‚å€¼å¾—ä¸€æçš„æ˜¯è¯¥åº”ç”¨ä½¿ç”¨ä¸¤ç§å‘èµ·æ¨æ–­çš„æ–¹æ³•ï¼Œåˆ†åˆ«æ˜¯ä½¿ç”¨Visionçš„requestå’Œä½¿ç”¨plain CoreML
``` swift
 func predict(pixelBuffer: CVPixelBuffer, inflightIndex: Int) {
    // Measure how long it takes to predict a single video frame.
    let startTime = CACurrentMediaTime()

    // This is an alternative way to resize the image (using vImage):
    //if let resizedPixelBuffer = resizePixelBuffer(pixelBuffer,
    //                                              width: YOLO.inputWidth,
    //                                              height: YOLO.inputHeight) {

    // Resize the input with Core Image to 416x416.
    if let resizedPixelBuffer = resizedPixelBuffers[inflightIndex] {
      let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
      let sx = CGFloat(YOLO.inputWidth) / CGFloat(CVPixelBufferGetWidth(pixelBuffer))
      let sy = CGFloat(YOLO.inputHeight) / CGFloat(CVPixelBufferGetHeight(pixelBuffer))
      let scaleTransform = CGAffineTransform(scaleX: sx, y: sy)
      let scaledImage = ciImage.transformed(by: scaleTransform)
      ciContext.render(scaledImage, to: resizedPixelBuffer)

      // Give the resized input to our model.
      if let boundingBoxes = yolo.predict(image: resizedPixelBuffer) {
        let elapsed = CACurrentMediaTime() - startTime
        showOnMainThread(boundingBoxes, elapsed)
      } else {
        print("BOGUS")
      }
    }

    self.semaphore.signal()
  }
```
plain CoreMLç›´æ¥è°ƒç”¨æ¨¡å‹çš„predictæ–¹æ³•ã€‚

``` swift
func predictUsingVision(pixelBuffer: CVPixelBuffer, inflightIndex: Int) {
    // Measure how long it takes to predict a single video frame. Note that
    // predict() can be called on the next frame while the previous one is
    // still being processed. Hence the need to queue up the start times.
    startTimes.append(CACurrentMediaTime())

    // Vision will automatically resize the input image.
    let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer)
    let request = requests[inflightIndex]

    // Because perform() will block until after the request completes, we
    // run it on a concurrent background queue, so that the next frame can
    // be scheduled in parallel with this one.
    DispatchQueue.global().async {
      try? handler.perform([request])
    }
  }

  func visionRequestDidComplete(request: VNRequest, error: Error?) {
    if let observations = request.results as? [VNCoreMLFeatureValueObservation],
       let features = observations.first?.featureValue.multiArrayValue {

      let boundingBoxes = yolo.computeBoundingBoxes(features: features)
      let elapsed = CACurrentMediaTime() - startTimes.remove(at: 0)
      showOnMainThread(boundingBoxes, elapsed)
    } else {
      print("BOGUS!")
    }

    self.semaphore.signal()
  }
```

ä½¿ç”¨Visionéœ€è¦åˆå§‹åŒ–requestï¼Œå¹¶ä¸ºå…¶è®¾ç½®handlerï¼Œç„¶ååœ¨å›¾åƒåˆ°æ¥æ—¶è¿›è¡Œhandlerçš„performï¼Œè§¦å‘å›è°ƒå‡½æ•°visionRequestDidCompleteã€‚åœ¨å›è°ƒå‡½æ•°ä¸­å¤„ç†è¿”å›å€¼ã€‚

ä¸¤ç§æ–¹å¼éƒ½å¯ä»¥ã€‚

+ é¡¹ç›®æ€»ç»“ï¼šè¯¥é¡¹ç›®åŒ…å«äº†å®Œæ•´çš„æ”¶é›†æ•°æ® ->  è®­ç»ƒæ¨¡å‹ -> æ¨¡å‹è½¬åŒ– -> æ¨¡å‹éƒ¨ç½²çš„è¿‡ç¨‹ã€‚ä½†æ˜¯æ•°æ®é›†è¾ƒå¤§ï¼Œæ¨¡å‹è®­ç»ƒéš¾åº¦è¾ƒé«˜ï¼Œå¯ä»¥è€ƒè™‘ç”¨è§„æ¨¡æ›´å°çš„æ•°æ®é›†å’Œæ›´ç®€å•çš„æ¨¡å‹ã€‚


## 6. æ–‡æœ¬åˆ†ç±»
+ é¡¹ç›®åœ°å€ï¼šhttps://github.com/cocoa-ai/SentimentCoreMLDemo
+ å…³é”®è¯ï¼š CoreML,  sklearn, coremltools, æ–‡æœ¬åˆ†ç±»
+ é¡¹ç›®æ•ˆæœï¼š
![](img/1AD96A82-3E27-4A42-9C2B-CBE46714FC56.png
)
+ åŠŸèƒ½æè¿°ï¼šæ ¹æ®æ–‡æœ¬çš„æƒ…æ„Ÿå°†æ–‡æœ¬åˆ†æˆç§¯æçš„(postive)å’Œæ¶ˆæçš„(negative)ä¸¤ç±»ã€‚
+ æŠ€æœ¯æè¿°ï¼šè¯¥æ–‡æœ¬åˆ†ç±»æ¨¡å‹ä½¿ç”¨çš„æ•°æ®é›†æ˜¯ [Homework:  Sentiment Analysis](http://boston.lti.cs.cmu.edu/classes/95-865-K/HW/HW3/)çš„HW3ä¸­çš„æ•°æ®é›†ï¼Œæœ‰1392æ®µçŸ­æ–‡æœ¬ï¼Œè¢«æ ‡æ³¨äº†Postiveæˆ–è€…Negativeçš„æ ‡ç­¾ã€‚ä½¿ç”¨sklearnä¸­çš„LinearSVCè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡Appleæä¾›çš„coremltoolså·¥å…·åŒ…å°†æ¨¡å‹è½¬åŒ–æˆå¯ä»¥ä¾›iOS appä½¿ç”¨çš„mlmodelæ ¼å¼ã€‚è¿™ä¸ªæ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªDict(String->Double)ï¼Œç»™å‡ºäº†æ–‡æœ¬çš„è¯é¢‘è¡¨ç¤ºã€‚è¾“å‡ºæ˜¯Dict(String->Double)ï¼Œç»™å‡ºäº†ç±»åˆ«å’Œæ¦‚ç‡ã€‚è¿›è¡Œæ¨æ–­çš„è¿‡ç¨‹æ˜¯æ¯”è¾ƒç®€å•çš„ï¼Œç›´æ¥è°ƒç”¨æ¨¡å‹ç±»çš„predictionæ–¹æ³•å°±å¯ä»¥ï¼š
``` swift
func predictSentiment(from text: String) -> Sentiment {
    do {
      let inputFeatures = features(from: text)
      // Make prediction only with 2 or more words
      guard inputFeatures.count > 1 else {
        throw Error.featuresMissing
      }

      let output = try model.prediction(input: inputFeatures)

      switch output.classLabel {
      case "Pos":
        return .positive
      case "Neg":
        return .negative
      default:
        return .neutral
      }
    } catch {
      return .neutral
    }
  }
```

+ é¡¹ç›®æ€»ç»“ï¼š æ¯”è¾ƒç®€å•çš„é¡¹ç›®ï¼Œä½¿ç”¨çš„æ•°æ®é›†ä¹Ÿå¾ˆå°ï¼Œä½†æ˜¯åŒ…å«äº†å®Œæ•´çš„ è®­ç»ƒ -> åº”ç”¨çš„è¿‡ç¨‹ã€‚å¯ä»¥ä»¿ç…§è¿™ä¸ªæ€è·¯ï¼Œæ‰¾ä¸€äº›å¤§çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨æ›´å¼ºå¤§çš„æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œç„¶åå†ç”¨coremltoolsæ¥è½¬åŒ–éƒ¨ç½²ã€‚

## 7. åŠ¨ä½œæ•æ‰
+ é¡¹ç›®åœ°å€ï¼š https://github.com/akimach/GestureAI-CoreML-iOS
+ å…³é”®è¯ï¼š CoreMLï¼ŒCoreMotion
+ é¡¹ç›®æ•ˆæœï¼š
![](img/3F995D9C-2AF3-4810-964D-0DE3B14F2244.png
)

+ åŠŸèƒ½æè¿°ï¼š æŒ‰ä½åº”ç”¨ä¸­çš„æŒ‰é’®ï¼Œç„¶åæ‹¿ç€æ‰‹æœºåšåŠ¨ä½œï¼Œåº”ç”¨å¯ä»¥è¯†åˆ«å‡ºæ‰‹æœºçš„è¿åŠ¨è½¨è¿¹ï¼ˆåœ†ã€çŸ©å½¢ã€ä¸‰è§’å½¢ï¼‰
+ æŠ€æœ¯æè¿°ï¼š å½“ç”¨æˆ·æŒ‰ä¸‹æŒ‰é’®æ—¶ï¼Œä¼šä¸æ–­ä»MotionManageråˆè·å–è¿åŠ¨ä¼ æ„Ÿå™¨çš„æ•°æ®ï¼š
``` swift
 @IBAction func gaBtnTouchDown(_ sender: Any) {
        gaBtn.backgroundColor = GAColor.btnSensing
        self.sequenceTarget = []
        
        timer = Timer.scheduledTimer(timeInterval: 1.0, target: self, selector: #selector(self.updateTimer(tm:)), userInfo: nil, repeats: true)
        timer.fire()
        
        motionManager.startAccelerometerUpdates(to: queue, withHandler: {
            (accelerometerData, error) in
            if let e = error {
                fatalError(e.localizedDescription)
            }
            guard let data = accelerometerData else { return }
            self.sequenceTarget.append(data.acceleration.x)
            self.sequenceTarget.append(data.acceleration.y)
            self.sequenceTarget.append(data.acceleration.z)
        })
    }
```
å½“æŒ‰é’®æ¾èµ·æ—¶ï¼Œä¼šå°†é‡‡é›†åˆ°çš„åŠ é€Ÿä¼ æ„Ÿå™¨åºåˆ—æ•°æ®ä¼ å…¥æ¨¡å‹ï¼Œè¿›è¡Œæ¨æ–­ã€‚

+ é¡¹ç›®æ€»ç»“ï¼š å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨CoreMotionè·å–è¿åŠ¨ä¼ æ„Ÿå™¨æ•°æ®ã€‚


## 8. ARç”»åˆ·
+ é¡¹ç›®åœ°å€ï¼šhttps://github.com/laanlabs/ARBrush
+ å…³é”®å­—ï¼š ARKit, metal, SceneKit
+ é¡¹ç›®æ•ˆæœï¼š
![](img/CFF1C9C4-B43B-471F-BDE9-F4D4DC6579AF.png
)
+ åŠŸèƒ½ä»‹ç»ï¼š  å¯ä»¥é€šè¿‡æŒ‰é’®æ¥å¢åŠ æ™¯æ·±ï¼Œç„¶åæŒ‰ä½å±å¹•å¹¶ç§»åŠ¨æ‰‹æœºï¼Œå°±å¯ä»¥ç”»å‡ºæƒ³è¦ç”»çš„å›¾å½¢ã€‚
+ é¡¹ç›®æ€»ç»“ï¼š ä»£ç æœ‰ç‚¹å¤æ‚ï¼Œéœ€è¦å­¦ä¹ ç›¸å…³çŸ¥è¯†åæ‰èƒ½ç†è§£ä»£ç è¿ä½œçš„æµç¨‹ã€‚


## 9. ARè¯†ç‰©
+ é¡¹ç›®åœ°å€ï¼š https://github.com/hanleyweng/CoreML-in-ARKit
+ å…³é”®è¯ï¼š CoreMLï¼Œ ARKitï¼ŒSceneKit
+ é¡¹ç›®æ•ˆæœï¼š 
![](img/E0C522DD-A541-48EA-B0E8-DC8293FE0A82.png
)

+ åŠŸèƒ½ä»‹ç»ï¼šç‚¹å‡»å±å¹•ï¼Œå°†è¯†åˆ«åˆ°çš„ç‰©ä½“çš„æ ‡ç­¾æ”¾ç½®åœ¨å±å¹•ä¸­å¿ƒæŒ‡å®šçš„åœºæ™¯ä½ç½®ã€‚
+ æŠ€æœ¯ä»‹ç»ï¼šä¸»è¦æ˜¯çœ‹ç‚¹å‡»å±å¹•æ—¶çš„å¤„ç†å‡½æ•°ï¼š
``` swift
@objc func handleTap(gestureRecognize: UITapGestureRecognizer) {
        // HIT TEST : REAL WORLD
        // Get Screen Centre
        let screenCentre : CGPoint = CGPoint(x: self.sceneView.bounds.midX, y: self.sceneView.bounds.midY)
        
        let arHitTestResults : [ARHitTestResult] = sceneView.hitTest(screenCentre, types: [.featurePoint]) // Alternatively, we could use '.existingPlaneUsingExtent' for more grounded hit-test-points.
        
        if let closestResult = arHitTestResults.first {
            // Get Coordinates of HitTest
            let transform : matrix_float4x4 = closestResult.worldTransform
            let worldCoord : SCNVector3 = SCNVector3Make(transform.columns.3.x, transform.columns.3.y, transform.columns.3.z)
            
            // Create 3D Text
            let node : SCNNode = createNewBubbleParentNode(latestPrediction)
            sceneView.scene.rootNode.addChildNode(node)
            node.position = worldCoord
        }
    }
```

è¿™é‡Œé€šè¿‡SceneKitè·å¾—æ ‡ç­¾åº”è¯¥è¢«æ”¾ç½®çš„ä¸‰ç»´åæ ‡ï¼Œç„¶åå°†latestPredictionåˆ¶ä½œæˆ3D Textçš„SceneNodeæ”¾ç½®åˆ°åœºæ™¯ä¸­ã€‚

+ é¡¹ç›®æ€»ç»“ï¼š ç»“åˆäº†CoreMLã€ARKitå’ŒSceneKitçš„åº”ç”¨ï¼Œä¼¼ä¹ARKitç»å¸¸æ˜¯å’ŒSceneKitæ”¾åœ¨ä¸€èµ·ä½¿ç”¨ã€‚


## 10. ARå é«˜å¡”æ¸¸æˆ
+ é¡¹ç›®åœ°å€ï¼š https://github.com/XanderXu/ARStack
+ å…³é”®è¯ï¼š ARKit SceneKit
+ é¡¹ç›®æ•ˆæœï¼š
[ARStack/1605417e98d57f47.gif at master Â· XanderXu/ARStack Â· GitHub](https://github.com/XanderXu/ARStack/blob/master/1605417e98d57f47.gif)







